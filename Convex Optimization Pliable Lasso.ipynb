{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pliable Lasso in Python\n",
    "\n",
    "This will all use the same example found in the pliable lasso paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxpy as cvx\n",
    "\n",
    "import matplotlib.pyplot as graph\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def v2a(a):\n",
    "    return a.reshape((len(a), 1))\n",
    "\n",
    "\n",
    "graph.style.use('fivethirtyeight')\n",
    "print(cvx.__version__)  # Make sure it's 1.0.11 or greater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Classical Lasso\n",
    "\n",
    "$ y = X \\beta $\n",
    "\n",
    "$ J(\\beta, \\lambda) = ||X \\beta||_2^2 + \\lambda||\\beta||_1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake data\n",
    "n = 10000\n",
    "p = 50\n",
    "k = 4\n",
    "\n",
    "beta = np.zeros(p)\n",
    "beta[:4] = [2, -2, 2, 2]\n",
    "print(beta)\n",
    "\n",
    "x = stats.norm().rvs(size=(n, p))\n",
    "y = x @ beta\n",
    "y += stats.norm().rvs(n)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_hat(x, beta):\n",
    "    return x @ beta\n",
    "\n",
    "def mse(x, y, beta):\n",
    "    return (0.5 * x.shape[0]) * cvx.norm(y_hat(x, beta) - y, p=2) ** 2\n",
    "\n",
    "def penalty(lam, beta):\n",
    "    return lam * cvx.norm(beta, p=1)\n",
    "\n",
    "def j(x, y, beta, lam):\n",
    "    return mse(x, y, beta) + penalty(lam, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "beta_hat = cvx.Variable(p)\n",
    "lam = cvx.Parameter(nonneg=True)\n",
    "problem = cvx.Problem(cvx.Minimize(\n",
    "    j(x_train, y_train, beta_hat, lam)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lambdas = np.logspace(1, 5, 20)  # I know this is a list of large lambda values.\n",
    "\n",
    "test_scores = []\n",
    "beta_path = []\n",
    "for lam_i in lambdas:\n",
    "    lam.value = lam_i\n",
    "    problem_result = problem.solve()\n",
    "    \n",
    "    test_scores.append(r2_score(y_test, x_test @ beta_hat.value))\n",
    "    beta_path.append(beta_hat.value)\n",
    "    \n",
    "beta_path = np.vstack(beta_path)\n",
    "print(beta_path.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.figure(figsize=(12, 5))\n",
    "graph.plot(lambdas, test_scores)\n",
    "graph.xscale('log')\n",
    "graph.show()\n",
    "\n",
    "graph.figure(figsize=(12, 5))\n",
    "for p_i in range(p):\n",
    "    graph.plot(lambdas, beta_path[:, p_i], linewidth=2)\n",
    "graph.xscale('log')\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the Pliable Lasso\n",
    "\n",
    "$ y = \\beta_0 1 + Z \\theta_0 + \\sum X_j(\\beta_j 1 + Z \\theta_j) $\n",
    "\n",
    "$ J(\\beta_0, \\theta_0, \\beta, \\theta) = {{1}\\over{2N}} \\sum$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = 0\n",
    "theta_0 = np.zeros(k)\n",
    "\n",
    "beta = np.zeros(p)\n",
    "beta[:4] = [2, -2, 2, 2]\n",
    "\n",
    "theta = np.zeros((p, k))\n",
    "theta[2, 0] = 2.0\n",
    "theta[3, 1] = -2.0\n",
    "\n",
    "z = stats.bernoulli(p=0.5).rvs(size=(n, k))\n",
    "print(z.shape)\n",
    "\n",
    "y = x[:, 0] * beta[0]\n",
    "y += x[:, 1] * beta[1]\n",
    "y += x[:, 2] * (beta[2] + 2*z[:, 0])\n",
    "y += x[:, 3] * (beta[3] - 2*z[:, 1])\n",
    "\n",
    "y_gt = y.copy()\n",
    "# y += 0.5 * stats.norm().rvs(n)  # Add noise from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_properties(obj, name=''):\n",
    "    print(name, f'DCP {obj.is_dcp()}', f'Convexity {obj.is_convex()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def compute_w_j(x, z, j):\n",
    "    k = z.shape[1]\n",
    "    x_j = np.tile(v2a(x[:, j]), (1, k))\n",
    "    return x_j * z\n",
    "\n",
    "def y_hat(beta_0, theta_0, beta, theta, x, z):  # Confirmed DCP and Convex\n",
    "    n = x.shape[0]\n",
    "    p = x.shape[1]\n",
    "    k = z.shape[1]\n",
    "    print(f'n {n}, p {p}, k {k}')\n",
    "    \n",
    "    intercepts = beta_0 + (z @ theta_0)\n",
    "    \n",
    "    shared_model = x @ beta\n",
    "    \n",
    "    pliables = np.zeros(n)\n",
    "    for j_i in range(p):\n",
    "        w_j = compute_w_j(x, z, j_i)\n",
    "        pliables = pliables + (w_j @ theta[j_i, :])\n",
    "        \n",
    "    output = intercepts + shared_model + pliables\n",
    "    return output\n",
    "\n",
    "def j(beta_0, theta_0, beta, theta, x, y, z, alpha, lam):\n",
    "    n, p = x.shape\n",
    "    \n",
    "    mse = (0.5*n) * cvx.sum((y - y_hat(beta_0, theta_0, beta, theta, x, z))**2)\n",
    "    \n",
    "    beta_matrix = cvx.reshape(beta, (p, 1))\n",
    "    \n",
    "    penalty_1 = cvx.sum(cvx.norm(cvx.hstack([beta_matrix, theta]), p=2, axis=1))\n",
    "    penalty_2 = cvx.sum(cvx.norm(theta, p=2, axis=1))\n",
    "    penalty_3 = cvx.sum(cvx.norm(theta, p=1))\n",
    "    \n",
    "#     loss = mse + (1-alpha) * lam * (penalty_1 + penalty_2) + alpha * lam * penalty_3\n",
    "    loss = mse + lam * (penalty_1 + penalty_2) + lam * penalty_3\n",
    "    check_properties(loss, 'loss')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "# Parameters\n",
    "beta_hat_0 = cvx.Variable(1)\n",
    "theta_hat_0 = cvx.Variable(k)\n",
    "beta_hat = cvx.Variable(p)\n",
    "theta_hat = cvx.Variable((p, k))\n",
    "\n",
    "print('beta_0', beta_hat_0.shape)\n",
    "print('theta_0', theta_hat_0.shape)\n",
    "print('beta', beta_hat.shape)\n",
    "print('theta', theta_hat.shape)\n",
    "print('='*20)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = cvx.Parameter(nonneg=True)\n",
    "alpha.value = 0.5\n",
    "lam = cvx.Parameter(nonneg=True)\n",
    "lam.value = 10**(2)\n",
    "\n",
    "# Solve\n",
    "problem = cvx.Problem(\n",
    "    cvx.Minimize(j(beta_hat_0, theta_hat_0, beta_hat, theta_hat, x, y, z, alpha, lam))\n",
    ")\n",
    "soln = problem.solve()\n",
    "\n",
    "# Results\n",
    "beta_hat.value[beta_hat.value < 1e-7] = 0  # Polish\n",
    "graph.plot(beta_hat.value)\n",
    "graph.show()\n",
    "\n",
    "theta_hat.value[theta_hat.value < 1e-7] = 0  # Polish\n",
    "graph.figure(figsize=(5, 10))\n",
    "sns.heatmap(theta_hat.value, annot=True)\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
